{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "07e8a22b-738a-4ccb-9e3b-f39dd0e653f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85548d3e-dbb8-4c2b-821d-c32db6a0e4f3",
   "metadata": {},
   "source": [
    "# Get Data (Shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "409cc6a9-0837-453f-8b71-b25894d39517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store text in one long string called 'data'\n",
    "f = open(\"./shakespeare.txt\")\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "369d9051-9a12-43f7-af76-025041fbe5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary to one-hot encode each characters to\n",
    "char2int = dict([(c, i) for i, c in enumerate(set(data))])\n",
    "int2char = dict([(char2int[k], k) for k in char2int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e764fe4a-eade-46fb-92d4-3822080163b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W': 0, 'f': 1, 'D': 2, 'v': 3, 'g': 4, 'M': 5, 'V': 6, \"'\": 7, 'L': 8, 'A': 9, 'z': 10, 'n': 11, '(': 12, 'k': 13, '-': 14, ';': 15, 'o': 16, 'J': 17, 'h': 18, 'q': 19, 'N': 20, 'x': 21, 'd': 22, 'b': 23, 'm': 24, 'E': 25, ':': 26, 's': 27, ' ': 28, 'S': 29, 'a': 30, 'C': 31, 'R': 32, 'T': 33, '!': 34, 'B': 35, 'I': 36, 'Y': 37, 'K': 38, 'j': 39, 'O': 40, '.': 41, 'u': 42, 'H': 43, ')': 44, 'y': 45, 'i': 46, 'p': 47, 't': 48, 'l': 49, '?': 50, 'w': 51, 'c': 52, ',': 53, '\\n': 54, 'P': 55, 'G': 56, 'e': 57, 'r': 58, 'U': 59, 'F': 60}\n",
      "{0: 'W', 1: 'f', 2: 'D', 3: 'v', 4: 'g', 5: 'M', 6: 'V', 7: \"'\", 8: 'L', 9: 'A', 10: 'z', 11: 'n', 12: '(', 13: 'k', 14: '-', 15: ';', 16: 'o', 17: 'J', 18: 'h', 19: 'q', 20: 'N', 21: 'x', 22: 'd', 23: 'b', 24: 'm', 25: 'E', 26: ':', 27: 's', 28: ' ', 29: 'S', 30: 'a', 31: 'C', 32: 'R', 33: 'T', 34: '!', 35: 'B', 36: 'I', 37: 'Y', 38: 'K', 39: 'j', 40: 'O', 41: '.', 42: 'u', 43: 'H', 44: ')', 45: 'y', 46: 'i', 47: 'p', 48: 't', 49: 'l', 50: '?', 51: 'w', 52: 'c', 53: ',', 54: '\\n', 55: 'P', 56: 'G', 57: 'e', 58: 'r', 59: 'U', 60: 'F'}\n"
     ]
    }
   ],
   "source": [
    "print(char2int)\n",
    "print(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d95e51a5-6a92-471d-a22c-47fc9c68e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode and move to Tensors\n",
    "x_encoded = nn.functional.one_hot((torch.Tensor([char2int[x] for x in data]).long()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "73184e4f-51c7-405f-a54d-cf353b6e901c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of reverting tensor back to single char for use later\n",
    "int2char[np.argmax(x_encoded[1].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ba8bc739-5312-4d0b-bcef-0ae896e219a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([94275, 61])\n"
     ]
    }
   ],
   "source": [
    "print(x_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112fb62-9ed1-4a51-9be7-7398235b6c01",
   "metadata": {},
   "source": [
    "# Build RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8fad76b8-edeb-47ca-ad8d-14dc6417a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.in_linear = nn.Linear(61, 100, bias=False)\n",
    "        self.hid_linear = nn.Linear(self.hidden_dim, 100)\n",
    "        self.out_linear = nn.Linear(100, 61)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        if hidden == None:\n",
    "            hidden = torch.zeros(self.hidden_dim).to(device)\n",
    "        ht = nn.functional.tanh(self.in_linear(x) + self.hid_linear(hidden))\n",
    "        ot = self.out_linear(ht)\n",
    "        return ht, ot\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c16d3365-0fc6-4b4e-a7db-16cf52ac89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbedcb9-a5b7-43ce-989c-5f7850f7d696",
   "metadata": {},
   "source": [
    "# Set System Standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "494ca9bd-34b4-4c0a-9020-25292afe50a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# set device for pytorch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# set loss function as cross entropy loss, optimizer and SGD\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bae6b78b-639a-4d64-b078-f77d379d6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move everything to CUDA\n",
    "model = model.to(device)\n",
    "x_encoded = x_encoded.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5ebbb760-c30f-4974-b6e8-5f73ba6328f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "def train(model, criterion, optimizer, data):\n",
    "\n",
    "    hidden = None\n",
    "    # set to train mode\n",
    "    model.train()\n",
    "    for i in trange(len(data)):\n",
    "        # skip first index, since at this point we have no context to train with\n",
    "        if i == 0:\n",
    "            continue\n",
    "\n",
    "        # label is the current char data[i] and input is the previous char data[i-1]\n",
    "        optimizer.zero_grad()\n",
    "        hidden, pred = model(data[i-1], None)\n",
    "        loss = criterion(pred, data[i])\n",
    "\n",
    "        # grad. descent\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b9b6aeb1-4e91-45a4-9e05-d8ac5c1fc148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 94275/94275 [01:51<00:00, 846.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 94275/94275 [01:51<00:00, 847.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 -----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 94275/94275 [01:51<00:00, 847.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 -----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 94275/94275 [01:51<00:00, 847.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 -----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 94275/94275 [01:50<00:00, 853.00it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t} -----------------------------------\")\n",
    "    train(model, criterion, optimizer, x_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339b22e-6984-4af5-a158-ee18cbebc9e1",
   "metadata": {},
   "source": [
    "# Generate Text from Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c49ff6bd-1291-4514-ab83-238fce04253d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 61])\n",
      "torch.Size([100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x100 and 200x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[138], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(generate_size):\n\u001b[1;32m---> 13\u001b[0m     hidden, next_chr \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(text\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(hidden\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[106], line 14\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, x, hidden)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 14\u001b[0m ht \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_linear(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhid_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     15\u001b[0m ot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_linear(ht)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ht, ot\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x100 and 200x100)"
     ]
    }
   ],
   "source": [
    "# switch to eval mode here to prevent gradients from being calculated\n",
    "model.eval()\n",
    "\n",
    "# extra characters to make\n",
    "generate_size = 100\n",
    "\n",
    "# keep running\n",
    "text = x_encoded[0]\n",
    "text = text.reshape(1, 61)\n",
    "\n",
    "hidden = None\n",
    "for i in range(generate_size):\n",
    "    hidden, next_chr = model(text[i], hidden)\n",
    "    print(text.shape)\n",
    "    print(hidden.shape)\n",
    "    text = torch.cat((text, next_chr.reshape(1, next_chr.shape[0])), dim=0)\n",
    "\n",
    "text_real = [int2char[np.argmax(text[i].tolist())] for i in range(len(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a38599-7830-47fb-bd60-94d92c636100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
